# Benchmarking SAT solvers in Go

[DRAFT]

In this article, I will present an overview of benchmarking SAT solvers in Go.
We will take a look at [gini](https://github.com/go-air/gini) and 
[gophersat](https://github.com/crillab/gophersat) on randomly selected 
[sat competition](http://satcompetition.org) problems from previous years.

## Background

The [sat problem](https://github.com/go-air/gini/docs/satprob.md) is a
canonical problem for the easiest intractable complexity class of problems, the
famous NP complete complexity class.  NP complete problems are characterised as
problems for which verifying whether or not a solution is correct is efficient,
but for which many such potential solutions exist.  

The Go tool suite comes with many performance aids, such as profiling and
micro benchmarks.  However, we have found that these tools are, perhaps with
good reason [^toohard], inappropriate for benchmarking hard combinatorial problems.
This is because the focus on benchmarking hard combinatorial problems is centered
around volume of different problems that can be solved, whereas Go's benchmarking
tooling is centered around the idea of average time to execute a given input.

These notions are completely different.  In the solver paradigm, we are focused
on how many problems can be solved from a large set of problems in a given time.
In micro-benchmark, subroutine problems, we are interested in the throughput of
a benchmarked subroutine.  Moreover, for competition problems, a majority of
the problems will be sufficiently difficult to challenge a majority of the 
competitors, so the problems are notably harder than many application domain 
problems.


## Tooling

Gini comes with a tool for benchmarking hard combinatorial problems, called `bench`.
Hard combinatorial problems are generally solved using heuristics and so they have
enormous variance in runtime when applied to different problems.  

Here is its usage:
```sh
bench -h
bench <cmd> [options] arg arg ...
<cmd> may be
	sel
	run
	cmp
For help with a command, run bench <cmd> -h.
```

`bench` can be used to select benchmarks, to run different solvers on those
benchmarks, and to compare the results.

The repository https://github.com/irifrance/suites contains some randomly selected
SAT competition problems from previous years.

We cloned this repository and then used `bench` to randomly select 35 problems
from them.

```
git clone https://github.com/irifrance/suites
bench sel -n 35 -name benchblog -pattern "*.cnf*"  suites
```

## Running the benchmarks

The `bench` tool runs and records the results of a benchmark for a benchmark
suite.

```
bench run -h
run [runoptions] suite [ suite [ suite ... ] ]
	run runs commands on benchmark suites enforcing timeouts and
	recording results.
  -cmd string
    	command to run on each instance
  -commit string
    	commit id of command
  -d string
    	delete the run
  -desc string
    	description of run.
  -dur duration
    	max per-instance duration (default 5s)
  -gdur duration
    	max run duration (default 1h0m0s)
  -name string
    	name of the run (default "run")
```

The `bench` tool is a bit taylored to SAT solvers, which exit code 10 on
satisfiable and code 20 on unsatisfiable, so that benchmarking tools don't need
to parse different outputs.  The "cmd" flag expects a command which follows
this (unconvential by unix standards) convention. Gini comes with a flag 
which implements this.

The "dur" flag gives a runtime duration per-instance.  The "gdur" flag gives
a total timeout, which can be useful if one expects a large proportion of
a suite to be solved in a given time and does not want the global duration
to equal the number of instances times the per-instance duration.

### Gini



### Gophersat


We took Gophersat at commit 72b19f5 and modified it as follows to 
fit the sat competition convention
```
diff --git a/main.go b/main.go
index f08b30c..806662f 100644
--- a/main.go
+++ b/main.go
@@ -66,7 +66,21 @@ func main() {
 			} else if count {
 				countModels(pb, verbose)
 			} else {
-				solve(pb, verbose, cert, printFn)
+				_ = printFn
+				solve(pb, verbose, cert, func(c chan solver.Result) {
+					res, ok := <-c
+					if !ok {
+						os.Exit(0)
+					}
+					switch res.Status {
+					case solver.Sat:
+						os.Exit(10)
+					case solver.Unsat:
+						os.Exit(20)
+					default:
+						os.Exit(0)
+					}
+				})
```

We used the following wrapper script to take care of the .bz2 inputs

```
#!/bin/sh

f=`mktemp`
gzip -d -c $@ > $f.cnf
exec gophersat $f.cnf
```

## Exploring the results

The `bench` tool facilitates exploring results with the cmp verb

```
bench cmp -h
cmp [cmp options] suite
  -cactus
    	cactus plot
  -list
    	list all instances in all runs.
  -runs string
    	comma separated list of run globs (default "*")
  -sat
    	list only sat instances in runs.
  -scatter
    	scatter plot of run pairs
  -sum
    	suite summary. (default true)
  -unsat
    	list only unsat instances in runs.
```





## Notes

[^toohard]: It is generally encouraged to avoid hard combinatorial problems
where possible in software development.  Such a practice guarantees that 
the resulting software is safe in the sense that it won't take exponential time
to complete some subtask.
